Epoch 1/30
21/21 [==============================] - 1s 24ms/step - loss: 1.6370 - acc: 0.3318 - val_loss: 1.4369 - val_acc: 0.2857
Epoch 2/30
21/21 [==============================] - 0s 19ms/step - loss: 1.1565 - acc: 0.5082 - val_loss: 2.0400 - val_acc: 0.3095
Epoch 3/30
14/21 [===================>..........] - ETA: 0s - loss: 0.9006 - acc: 0.6674
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:47: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('shade.When', 'shade. When')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:48: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('afraid.Now', 'afraid. Now')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:50: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('afraid.Now,', 'afraid. Now,')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:51: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('Big Game.Bigger', 'Big Game. Bigger')
[nltk_data] Downloading package stopwords to C:\Users\Jesli's
[nltk_data]     Laptop\AppData\Roaming\nltk_data...
21/21 [==============================] - 0s 20ms/step - loss: 1.0461 - acc: 0.6263 - val_loss: 1.9931 - val_acc: 0.3155
Epoch 4/30
21/21 [==============================] - 0s 19ms/step - loss: 1.0181 - acc: 0.6667 - val_loss: 2.2596 - val_acc: 0.4107
Epoch 5/30
21/21 [==============================] - 0s 19ms/step - loss: 0.9587 - acc: 0.6921 - val_loss: 2.6615 - val_acc: 0.4226
Epoch 6/30
21/21 [==============================] - 0s 19ms/step - loss: 0.9055 - acc: 0.7280 - val_loss: 3.4539 - val_acc: 0.3393
Epoch 7/30
21/21 [==============================] - 0s 19ms/step - loss: 0.9599 - acc: 0.7324 - val_loss: 3.4465 - val_acc: 0.3750
Epoch 8/30
21/21 [==============================] - 0s 20ms/step - loss: 1.2754 - acc: 0.7250 - val_loss: 4.3720 - val_acc: 0.2976
Epoch 9/30
21/21 [==============================] - 0s 19ms/step - loss: 1.1579 - acc: 0.7354 - val_loss: 5.8274 - val_acc: 0.3274
Epoch 10/30
21/21 [==============================] - 0s 19ms/step - loss: 1.1055 - acc: 0.7638 - val_loss: 4.9755 - val_acc: 0.3512
Epoch 11/30
21/21 [==============================] - 0s 19ms/step - loss: 0.9670 - acc: 0.7728 - val_loss: 5.6266 - val_acc: 0.3571
Epoch 12/30
21/21 [==============================] - 0s 19ms/step - loss: 0.9378 - acc: 0.8117 - val_loss: 5.7381 - val_acc: 0.3869
Epoch 13/30
21/21 [==============================] - 0s 19ms/step - loss: 0.7485 - acc: 0.8371 - val_loss: 6.1710 - val_acc: 0.3810
Epoch 14/30
21/21 [==============================] - 0s 19ms/step - loss: 1.0018 - acc: 0.8012 - val_loss: 5.7970 - val_acc: 0.3571
Epoch 15/30
21/21 [==============================] - 0s 21ms/step - loss: 0.9804 - acc: 0.8146 - val_loss: 7.0358 - val_acc: 0.3571
Epoch 16/30
21/21 [==============================] - 0s 19ms/step - loss: 1.0591 - acc: 0.8087 - val_loss: 7.2419 - val_acc: 0.3750
Epoch 17/30
21/21 [==============================] - 0s 19ms/step - loss: 1.1294 - acc: 0.8087 - val_loss: 8.8418 - val_acc: 0.3095
Epoch 18/30
21/21 [==============================] - 0s 19ms/step - loss: 1.0600 - acc: 0.8296 - val_loss: 8.9141 - val_acc: 0.3750
Epoch 19/30
21/21 [==============================] - 0s 19ms/step - loss: 1.2896 - acc: 0.8012 - val_loss: 8.9397 - val_acc: 0.4107
Epoch 20/30
21/21 [==============================] - 0s 19ms/step - loss: 1.2622 - acc: 0.8102 - val_loss: 8.6029 - val_acc: 0.3571
Epoch 21/30
21/21 [==============================] - 0s 20ms/step - loss: 1.0872 - acc: 0.8460 - val_loss: 9.8881 - val_acc: 0.3333
Epoch 22/30
21/21 [==============================] - 0s 20ms/step - loss: 1.3749 - acc: 0.8102 - val_loss: 11.5684 - val_acc: 0.3631
Epoch 23/30
21/21 [==============================] - 0s 21ms/step - loss: 1.5477 - acc: 0.8117 - val_loss: 12.5543 - val_acc: 0.3631
Epoch 24/30
21/21 [==============================] - 0s 20ms/step - loss: 1.6220 - acc: 0.8072 - val_loss: 11.6534 - val_acc: 0.3869
Epoch 25/30
21/21 [==============================] - 0s 21ms/step - loss: 1.8473 - acc: 0.8072 - val_loss: 12.7213 - val_acc: 0.3810
Epoch 26/30
21/21 [==============================] - 0s 21ms/step - loss: 1.5238 - acc: 0.8251 - val_loss: 13.7542 - val_acc: 0.3929
Epoch 27/30
21/21 [==============================] - 0s 20ms/step - loss: 1.2925 - acc: 0.8610 - val_loss: 14.0525 - val_acc: 0.3631
Epoch 28/30
21/21 [==============================] - 0s 22ms/step - loss: 1.4819 - acc: 0.8490 - val_loss: 13.6802 - val_acc: 0.3690
Epoch 29/30
21/21 [==============================] - 0s 22ms/step - loss: 1.6300 - acc: 0.8251 - val_loss: 15.4086 - val_acc: 0.3988
Epoch 30/30
21/21 [==============================] - 0s 21ms/step - loss: 1.2971 - acc: 0.8535 - val_loss: 14.8137 - val_acc: 0.3869
{'loss': [1.6369526386260986, 1.1565314531326294, 1.0461244583129883, 1.018096685409546, 0.9586981534957886, 0.9055285453796387, 0.959882378578186, 1.2754390239715576, 1.1579428911209106, 1.1055481433868408, 0.9669592976570129, 0.9378494024276733, 0.7484703063964844, 1.0017591714859009, 0.9804345369338989, 1.0591473579406738, 1.1293604373931885, 1.0599507093429565, 1.2896373271942139, 1.2622398138046265, 1.0871652364730835, 1.3748936653137207, 1.5476760864257812, 1.6219794750213623, 1.8473402261734009, 1.5237839221954346, 1.292526364326477, 1.481876015663147, 1.6299782991409302, 1.297128677368164], 'acc': [0.33183857798576355, 0.5082212090492249, 0.6263079047203064, 0.6666666865348816, 0.6920777559280396, 0.7279521822929382, 0.73243647813797, 0.7249626517295837, 0.7354260087013245, 0.7638266086578369, 0.7727952003479004, 0.8116592168807983, 0.8370702266693115, 0.8011958003044128, 0.8146487474441528, 0.8086696267127991, 0.8086696267127991, 0.8295964002609253, 0.8011958003044128, 0.8101644515991211, 0.8460388779640198, 0.8101644515991211, 0.8116592168807983, 0.8071748614311218, 0.8071748614311218, 0.8251121044158936, 0.8609865307807922, 0.8490284085273743, 0.8251121044158936, 0.853512704372406], 'val_loss': [1.4368681907653809, 2.0399534702301025, 1.9930634498596191, 2.2596116065979004, 2.661465883255005, 3.4538533687591553, 3.4465277194976807, 4.371952056884766, 5.8273606300354, 4.975466728210449, 5.626587390899658, 5.738101482391357, 6.170982837677002, 5.797033309936523, 7.035793304443359, 7.241926670074463, 8.841761589050293, 8.914104461669922, 8.939684867858887, 8.602947235107422, 9.888078689575195, 11.56840991973877, 12.554290771484375, 11.6533784866333, 12.721299171447754, 13.754158020019531, 14.052529335021973, 13.680158615112305, 15.40861701965332, 14.813736915588379], 'val_acc': [0.2857142984867096, 0.3095238208770752, 0.3154761791229248, 0.4107142984867096, 0.4226190447807312, 0.3392857015132904, 0.375, 0.2976190447807312, 0.3273809552192688, 0.3511904776096344, 0.3571428656578064, 0.386904776096344, 0.380952388048172, 0.3571428656578064, 0.3571428656578064, 0.375, 0.3095238208770752, 0.375, 0.4107142984867096, 0.3571428656578064, 0.3333333432674408, 0.363095223903656, 0.363095223903656, 0.386904776096344, 0.380952388048172, 0.3928571343421936, 0.363095223903656, 0.369047611951828, 0.3988095223903656, 0.386904776096344]}
5/5 [==============================] - 0s 4ms/step - loss: 17.0818 - acc: 0.2733