c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:47: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('shade.When', 'shade. When')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:48: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('afraid.Now', 'afraid. Now')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:50: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('afraid.Now,', 'afraid. Now,')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:51: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('Big Game.Bigger', 'Big Game. Bigger')
[nltk_data] Downloading package stopwords to C:\Users\Jesli's
[nltk_data]     Laptop\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Epoch 1/30
21/21 [==============================] - 1s 19ms/step - loss: 1.3558 - acc: 0.3109 - val_loss: 1.3690 - val_acc: 0.2917
Epoch 2/30
21/21 [==============================] - 0s 11ms/step - loss: 1.2712 - acc: 0.4469 - val_loss: 1.3217 - val_acc: 0.3810
Epoch 3/30
21/21 [==============================] - 0s 11ms/step - loss: 1.1378 - acc: 0.5217 - val_loss: 1.3140 - val_acc: 0.4107
Epoch 4/30
21/21 [==============================] - 0s 11ms/step - loss: 0.9841 - acc: 0.6203 - val_loss: 1.4095 - val_acc: 0.3810
Epoch 5/30
21/21 [==============================] - 0s 11ms/step - loss: 0.8748 - acc: 0.6622 - val_loss: 1.3147 - val_acc: 0.4048
Epoch 6/30
21/21 [==============================] - 0s 11ms/step - loss: 0.7654 - acc: 0.7175 - val_loss: 1.4589 - val_acc: 0.4107
Epoch 7/30
21/21 [==============================] - 0s 11ms/step - loss: 0.6757 - acc: 0.7608 - val_loss: 1.4470 - val_acc: 0.3810
Epoch 8/30
21/21 [==============================] - 0s 12ms/step - loss: 0.6082 - acc: 0.7803 - val_loss: 1.5527 - val_acc: 0.3750
Epoch 9/30
21/21 [==============================] - 0s 12ms/step - loss: 0.5425 - acc: 0.8027 - val_loss: 1.6445 - val_acc: 0.3690
Epoch 10/30
21/21 [==============================] - 0s 11ms/step - loss: 0.4939 - acc: 0.8266 - val_loss: 1.7322 - val_acc: 0.3631
Epoch 11/30
21/21 [==============================] - 0s 11ms/step - loss: 0.4513 - acc: 0.8341 - val_loss: 1.8128 - val_acc: 0.3571
Epoch 12/30
21/21 [==============================] - 0s 11ms/step - loss: 0.4483 - acc: 0.8386 - val_loss: 1.8983 - val_acc: 0.3690
Epoch 13/30
21/21 [==============================] - 0s 11ms/step - loss: 0.4044 - acc: 0.8595 - val_loss: 2.0354 - val_acc: 0.3810
Epoch 14/30
21/21 [==============================] - 0s 11ms/step - loss: 0.3582 - acc: 0.8849 - val_loss: 2.1139 - val_acc: 0.3810
Epoch 15/30
21/21 [==============================] - 0s 12ms/step - loss: 0.3438 - acc: 0.8909 - val_loss: 2.2453 - val_acc: 0.3512
Epoch 16/30
21/21 [==============================] - 0s 11ms/step - loss: 0.3178 - acc: 0.8909 - val_loss: 2.3191 - val_acc: 0.3810
Epoch 17/30
21/21 [==============================] - 0s 11ms/step - loss: 0.3044 - acc: 0.8894 - val_loss: 2.4077 - val_acc: 0.3810
Epoch 18/30
21/21 [==============================] - 0s 11ms/step - loss: 0.2829 - acc: 0.9073 - val_loss: 2.4908 - val_acc: 0.3810
Epoch 19/30
21/21 [==============================] - 0s 13ms/step - loss: 0.2484 - acc: 0.9193 - val_loss: 2.6574 - val_acc: 0.3452
Epoch 20/30
21/21 [==============================] - 0s 12ms/step - loss: 0.2661 - acc: 0.9028 - val_loss: 2.6873 - val_acc: 0.3869
Epoch 21/30
21/21 [==============================] - 0s 11ms/step - loss: 0.2399 - acc: 0.9148 - val_loss: 2.7840 - val_acc: 0.3512
Epoch 22/30
21/21 [==============================] - 0s 11ms/step - loss: 0.2348 - acc: 0.9193 - val_loss: 2.8476 - val_acc: 0.3810
Epoch 23/30
21/21 [==============================] - 0s 11ms/step - loss: 0.2179 - acc: 0.9238 - val_loss: 2.9666 - val_acc: 0.3750
Epoch 24/30
21/21 [==============================] - 0s 11ms/step - loss: 0.2172 - acc: 0.9253 - val_loss: 3.0586 - val_acc: 0.3690
Epoch 25/30
21/21 [==============================] - 0s 12ms/step - loss: 0.2171 - acc: 0.9238 - val_loss: 3.1237 - val_acc: 0.3631
Epoch 26/30
21/21 [==============================] - 0s 11ms/step - loss: 0.2070 - acc: 0.9297 - val_loss: 3.2481 - val_acc: 0.3452
Epoch 27/30
21/21 [==============================] - 0s 13ms/step - loss: 0.2135 - acc: 0.9297 - val_loss: 3.2957 - val_acc: 0.3512
Epoch 28/30
21/21 [==============================] - 0s 11ms/step - loss: 0.2100 - acc: 0.9283 - val_loss: 3.3105 - val_acc: 0.3690
Epoch 29/30
21/21 [==============================] - 0s 11ms/step - loss: 0.1921 - acc: 0.9342 - val_loss: 3.3413 - val_acc: 0.3690
Epoch 30/30
21/21 [==============================] - 0s 11ms/step - loss: 0.1865 - acc: 0.9268 - val_loss: 3.3952 - val_acc: 0.3571
{'loss': [1.3557945489883423, 1.2712337970733643, 1.1378117799758911, 0.9841220378875732, 0.8748316764831543, 0.7654136419296265, 0.6757111549377441, 0.6081585884094238, 0.5425299406051636, 0.4939087927341461, 0.45133116841316223, 0.4482845664024353, 0.40440648794174194, 0.35823753476142883, 0.34378066658973694, 0.31775376200675964, 0.3044026494026184, 0.2828969359397888, 0.24840952455997467, 0.26611682772636414, 0.23987826704978943, 0.23478569090366364, 0.21785522997379303, 0.2172151505947113, 0.21710717678070068, 0.20698589086532593, 0.21353696286678314, 0.20999640226364136, 0.19212615489959717, 0.18654842674732208], 'acc': [0.31091180443763733, 0.4469357132911682, 0.5216741561889648, 0.6203288435935974, 0.6621823906898499, 0.7174887657165527, 0.7608370780944824, 0.7802690863609314, 0.8026905655860901, 0.8266068696975708, 0.834080696105957, 0.8385650515556335, 0.859491765499115, 0.884902834892273, 0.8908818960189819, 0.8908818960189819, 0.8893871307373047, 0.9073243737220764, 0.9192824959754944, 0.9028400778770447, 0.9147982001304626, 0.9192824959754944, 0.9237667918205261, 0.9252615571022034, 0.9237667918205261, 0.9297459125518799, 0.9297459125518799, 0.9282511472702026, 0.9342302083969116, 0.9267563819885254], 'val_loss': [1.368991732597351, 1.3217114210128784, 1.313995361328125, 1.4095147848129272, 1.3146581649780273, 1.4589375257492065, 1.4469656944274902, 1.5526806116104126, 1.6445472240447998, 1.7321561574935913, 1.8127857446670532, 1.8982923030853271, 2.0354294776916504, 2.113859176635742, 2.245288610458374, 2.3191475868225098, 2.4076504707336426, 2.4907655715942383, 2.657438278198242, 2.687342643737793, 2.7839508056640625, 2.847559690475464, 2.966583728790283, 3.0585825443267822, 3.1236696243286133, 3.248147964477539, 3.2956836223602295, 3.31050443649292, 3.3412585258483887, 3.395232677459717], 'val_acc': [0.2916666567325592, 0.380952388048172, 0.4107142984867096, 0.380952388048172, 0.4047619104385376, 0.4107142984867096, 0.380952388048172, 0.375, 0.369047611951828, 0.363095223903656, 0.3571428656578064, 0.369047611951828, 0.380952388048172, 0.380952388048172, 0.3511904776096344, 0.380952388048172, 0.380952388048172, 0.380952388048172, 0.3452380895614624, 0.386904776096344, 0.3511904776096344, 0.380952388048172, 0.375, 0.369047611951828, 0.363095223903656, 0.3452380895614624, 0.3511904776096344, 0.369047611951828, 0.369047611951828, 0.3571428656578064]}
5/5 [==============================] - 0s 3ms/step - loss: 2.9129 - acc: 0.3333