Epoch 1/30
21/21 [==============================] - 1s 20ms/step - loss: 1.3796 - acc: 0.2422 - val_loss: 1.3737 - val_acc: 0.2738
Epoch 2/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3708 - acc: 0.2735 - val_loss: 1.3699 - val_acc: 0.2798
Epoch 3/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3672 - acc: 0.2915 - val_loss: 1.3688 - val_acc: 0.2500
Epoch 4/30
21/21 [==============================] - 0s 11ms/step - loss: 1.3661 - acc: 0.2855 - val_loss: 1.3677 - val_acc: 0.2619
Epoch 5/30
15/21 [====================>.........] - ETA: 0s - loss: 1.3547 - acc: 0.3042
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:47: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('shade.When', 'shade. When')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:48: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('afraid.Now', 'afraid. Now')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:50: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('afraid.Now,', 'afraid. Now,')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:51: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('Big Game.Bigger', 'Big Game. Bigger')
[nltk_data] Downloading package stopwords to C:\Users\Jesli's
[nltk_data]     Laptop\AppData\Roaming\nltk_data...
21/21 [==============================] - 0s 9ms/step - loss: 1.3635 - acc: 0.2945 - val_loss: 1.3674 - val_acc: 0.1964
Epoch 6/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3641 - acc: 0.3034 - val_loss: 1.3681 - val_acc: 0.2262
Epoch 7/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3638 - acc: 0.2810 - val_loss: 1.3683 - val_acc: 0.2262
Epoch 8/30
21/21 [==============================] - 0s 11ms/step - loss: 1.3628 - acc: 0.3049 - val_loss: 1.3675 - val_acc: 0.2262
Epoch 9/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3640 - acc: 0.2975 - val_loss: 1.3675 - val_acc: 0.2143
Epoch 10/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3627 - acc: 0.3064 - val_loss: 1.3677 - val_acc: 0.2321
Epoch 11/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3619 - acc: 0.3064 - val_loss: 1.3674 - val_acc: 0.2024
Epoch 12/30
21/21 [==============================] - 0s 11ms/step - loss: 1.3619 - acc: 0.2945 - val_loss: 1.3670 - val_acc: 0.2143
Epoch 13/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3614 - acc: 0.3064 - val_loss: 1.3664 - val_acc: 0.2143
Epoch 14/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3590 - acc: 0.3109 - val_loss: 1.3665 - val_acc: 0.2143
Epoch 15/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3611 - acc: 0.2960 - val_loss: 1.3676 - val_acc: 0.2083
Epoch 16/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3599 - acc: 0.2960 - val_loss: 1.3680 - val_acc: 0.2083
Epoch 17/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3603 - acc: 0.3109 - val_loss: 1.3677 - val_acc: 0.2202
Epoch 18/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3592 - acc: 0.3049 - val_loss: 1.3672 - val_acc: 0.2202
Epoch 19/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3592 - acc: 0.3184 - val_loss: 1.3680 - val_acc: 0.2083
Epoch 20/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3613 - acc: 0.3079 - val_loss: 1.3677 - val_acc: 0.2083
Epoch 21/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3588 - acc: 0.3154 - val_loss: 1.3676 - val_acc: 0.2024
Epoch 22/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3589 - acc: 0.3109 - val_loss: 1.3673 - val_acc: 0.2202
Epoch 23/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3589 - acc: 0.3064 - val_loss: 1.3673 - val_acc: 0.2083
Epoch 24/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3591 - acc: 0.3139 - val_loss: 1.3676 - val_acc: 0.2262
Epoch 25/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3585 - acc: 0.3034 - val_loss: 1.3673 - val_acc: 0.2143
Epoch 26/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3576 - acc: 0.3124 - val_loss: 1.3678 - val_acc: 0.2321
Epoch 27/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3571 - acc: 0.3034 - val_loss: 1.3679 - val_acc: 0.2321
Epoch 28/30
21/21 [==============================] - 0s 10ms/step - loss: 1.3566 - acc: 0.3214 - val_loss: 1.3692 - val_acc: 0.2381
Epoch 29/30
21/21 [==============================] - 0s 9ms/step - loss: 1.3558 - acc: 0.3244 - val_loss: 1.3692 - val_acc: 0.2381
Epoch 30/30
21/21 [==============================] - 0s 15ms/step - loss: 1.3560 - acc: 0.3184 - val_loss: 1.3687 - val_acc: 0.2321
{'loss': [1.3795843124389648, 1.37079656124115, 1.3671590089797974, 1.3661226034164429, 1.3635092973709106, 1.3640869855880737, 1.363821029663086, 1.3628302812576294, 1.364024043083191, 1.3627005815505981, 1.3618993759155273, 1.3618563413619995, 1.3614473342895508, 1.3590152263641357, 1.361141324043274, 1.3599419593811035, 1.3603153228759766, 1.3591750860214233, 1.359183430671692, 1.3612793684005737, 1.358846664428711, 1.3589224815368652, 1.3589085340499878, 1.3590655326843262, 1.358452558517456, 1.3576329946517944, 1.357110619544983, 1.3565963506698608, 1.3557652235031128, 1.3559714555740356], 'acc': [0.24215246737003326, 0.2735426127910614, 0.29147982597351074, 0.28550073504447937, 0.29446935653686523, 0.3034379780292511, 0.28101643919944763, 0.30493274331092834, 0.2974588871002197, 0.3064275085926056, 0.3064275085926056, 0.29446935653686523, 0.3064275085926056, 0.31091180443763733, 0.2959641218185425, 0.2959641218185425, 0.31091180443763733, 0.30493274331092834, 0.31838566064834595, 0.30792227387428284, 0.31539610028266907, 0.31091180443763733, 0.3064275085926056, 0.3139013350009918, 0.3034379780292511, 0.3124065697193146, 0.3034379780292511, 0.32137519121170044, 0.32436472177505493, 0.31838566064834595], 'val_loss': [1.3736984729766846, 1.3698922395706177, 1.3688043355941772, 1.3676823377609253, 1.367415189743042, 1.368146300315857, 1.3682687282562256, 1.367485523223877, 1.3675357103347778, 1.3677312135696411, 1.367353916168213, 1.3669898509979248, 1.366405725479126, 1.3664711713790894, 1.367554783821106, 1.3680466413497925, 1.3677406311035156, 1.3672288656234741, 1.3680181503295898, 1.367729902267456, 1.3676437139511108, 1.3673040866851807, 1.3673484325408936, 1.3676073551177979, 1.3673237562179565, 1.367780089378357, 1.3679108619689941, 1.3691819906234741, 1.369170069694519, 1.3686975240707397], 'val_acc': [0.2738095223903656, 0.2797619104385376, 0.25, 0.261904776096344, 0.1964285671710968, 0.2261904776096344, 0.2261904776096344, 0.2261904776096344, 0.2142857164144516, 0.2321428507566452, 0.2023809552192688, 0.2142857164144516, 0.2142857164144516, 0.2142857164144516, 0.2083333283662796, 0.2083333283662796, 0.2202380895614624, 0.2202380895614624, 0.2083333283662796, 0.2083333283662796, 0.2023809552192688, 0.2202380895614624, 0.2083333283662796, 0.2261904776096344, 0.2142857164144516, 0.2321428507566452, 0.2321428507566452, 0.2380952388048172, 0.2380952388048172, 0.2321428507566452]}
5/5 [==============================] - 0s 5ms/step - loss: 1.6191 - acc: 0.0867