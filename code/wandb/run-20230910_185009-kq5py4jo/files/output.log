Epoch 1/30
21/21 [==============================] - 1s 12ms/step - loss: 1.3784 - acc: 0.2945 - val_loss: 1.3785 - val_acc: 0.2202
Epoch 2/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3703 - acc: 0.2990 - val_loss: 1.3767 - val_acc: 0.2202
Epoch 3/30
21/21 [==============================] - 0s 7ms/step - loss: 1.3680 - acc: 0.2975 - val_loss: 1.3763 - val_acc: 0.2202
Epoch 4/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3663 - acc: 0.2930 - val_loss: 1.3765 - val_acc: 0.2202
Epoch 5/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3649 - acc: 0.3004 - val_loss: 1.3761 - val_acc: 0.2202
Epoch 6/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3635 - acc: 0.3019 - val_loss: 1.3751 - val_acc: 0.2262
Epoch 7/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3634 - acc: 0.3094 - val_loss: 1.3752 - val_acc: 0.2262
Epoch 8/30
21/21 [==============================] - 0s 7ms/step - loss: 1.3638 - acc: 0.2960 - val_loss: 1.3748 - val_acc: 0.2262
Epoch 9/30
 1/21 [>.............................] - ETA: 0s - loss: 1.3195 - acc: 0.2500
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:47: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('shade.When', 'shade. When')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:48: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('afraid.Now', 'afraid. Now')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:50: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('afraid.Now,', 'afraid. Now,')
c:\Users\Jesli's Laptop\Desktop\Acads\MLO\mlo-final-project\code\model.py:51: FutureWarning: The default value of regex will change from True to False in a future version.
  self.df_train['Poem'] = self.df_train['Poem'].str.replace('Big Game.Bigger', 'Big Game. Bigger')
[nltk_data] Downloading package stopwords to C:\Users\Jesli's
[nltk_data]     Laptop\AppData\Roaming\nltk_data...
21/21 [==============================] - 0s 7ms/step - loss: 1.3627 - acc: 0.3094 - val_loss: 1.3754 - val_acc: 0.2262
Epoch 10/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3646 - acc: 0.2975 - val_loss: 1.3738 - val_acc: 0.2262
Epoch 11/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3618 - acc: 0.3094 - val_loss: 1.3744 - val_acc: 0.2321
Epoch 12/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3630 - acc: 0.3034 - val_loss: 1.3762 - val_acc: 0.2262
Epoch 13/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3612 - acc: 0.3109 - val_loss: 1.3750 - val_acc: 0.2321
Epoch 14/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3608 - acc: 0.3019 - val_loss: 1.3747 - val_acc: 0.2321
Epoch 15/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3600 - acc: 0.3049 - val_loss: 1.3735 - val_acc: 0.2262
Epoch 16/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3612 - acc: 0.3034 - val_loss: 1.3733 - val_acc: 0.2321
Epoch 17/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3597 - acc: 0.3094 - val_loss: 1.3744 - val_acc: 0.2500
Epoch 18/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3613 - acc: 0.2975 - val_loss: 1.3732 - val_acc: 0.2440
Epoch 19/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3622 - acc: 0.2915 - val_loss: 1.3725 - val_acc: 0.2440
Epoch 20/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3607 - acc: 0.3109 - val_loss: 1.3716 - val_acc: 0.2500
Epoch 21/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3606 - acc: 0.3034 - val_loss: 1.3736 - val_acc: 0.2500
Epoch 22/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3580 - acc: 0.3049 - val_loss: 1.3726 - val_acc: 0.2321
Epoch 23/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3606 - acc: 0.2990 - val_loss: 1.3724 - val_acc: 0.2440
Epoch 24/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3596 - acc: 0.2930 - val_loss: 1.3731 - val_acc: 0.2440
Epoch 25/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3596 - acc: 0.3139 - val_loss: 1.3726 - val_acc: 0.2440
Epoch 26/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3592 - acc: 0.2990 - val_loss: 1.3729 - val_acc: 0.2321
Epoch 27/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3599 - acc: 0.3034 - val_loss: 1.3726 - val_acc: 0.2500
Epoch 28/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3565 - acc: 0.2975 - val_loss: 1.3714 - val_acc: 0.2500
Epoch 29/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3581 - acc: 0.3079 - val_loss: 1.3725 - val_acc: 0.2262
Epoch 30/30
21/21 [==============================] - 0s 6ms/step - loss: 1.3590 - acc: 0.3004 - val_loss: 1.3721 - val_acc: 0.2321
{'loss': [1.37838613986969, 1.370327353477478, 1.3680464029312134, 1.3662830591201782, 1.3649100065231323, 1.3634508848190308, 1.3633999824523926, 1.3638415336608887, 1.3626651763916016, 1.3646037578582764, 1.3618040084838867, 1.3630061149597168, 1.3611667156219482, 1.3608275651931763, 1.3600212335586548, 1.361228108406067, 1.3597058057785034, 1.3613454103469849, 1.3621619939804077, 1.360687255859375, 1.3606243133544922, 1.358016848564148, 1.360595464706421, 1.3595924377441406, 1.3596488237380981, 1.3592063188552856, 1.3598626852035522, 1.3565179109573364, 1.3581292629241943, 1.3589645624160767], 'acc': [0.29446935653686523, 0.298953652381897, 0.2974588871002197, 0.292974591255188, 0.3004484176635742, 0.30194321274757385, 0.3094170391559601, 0.2959641218185425, 0.3094170391559601, 0.2974588871002197, 0.3094170391559601, 0.3034379780292511, 0.31091180443763733, 0.30194321274757385, 0.30493274331092834, 0.3034379780292511, 0.3094170391559601, 0.2974588871002197, 0.29147982597351074, 0.31091180443763733, 0.3034379780292511, 0.30493274331092834, 0.298953652381897, 0.292974591255188, 0.3139013350009918, 0.298953652381897, 0.3034379780292511, 0.2974588871002197, 0.30792227387428284, 0.3004484176635742], 'val_loss': [1.3784730434417725, 1.3766926527023315, 1.3763048648834229, 1.3764749765396118, 1.3760656118392944, 1.3751028776168823, 1.3751612901687622, 1.3748400211334229, 1.3753974437713623, 1.3737961053848267, 1.3744417428970337, 1.3762446641921997, 1.3750133514404297, 1.3746802806854248, 1.3734949827194214, 1.3732739686965942, 1.3744192123413086, 1.373246192932129, 1.3725239038467407, 1.3716055154800415, 1.373621940612793, 1.372624397277832, 1.3724017143249512, 1.3730710744857788, 1.372621774673462, 1.3728972673416138, 1.3725961446762085, 1.3714481592178345, 1.3724762201309204, 1.3721346855163574], 'val_acc': [0.2202380895614624, 0.2202380895614624, 0.2202380895614624, 0.2202380895614624, 0.2202380895614624, 0.2261904776096344, 0.2261904776096344, 0.2261904776096344, 0.2261904776096344, 0.2261904776096344, 0.2321428507566452, 0.2261904776096344, 0.2321428507566452, 0.2321428507566452, 0.2261904776096344, 0.2321428507566452, 0.25, 0.244047611951828, 0.244047611951828, 0.25, 0.25, 0.2321428507566452, 0.244047611951828, 0.244047611951828, 0.244047611951828, 0.2321428507566452, 0.25, 0.25, 0.2261904776096344, 0.2321428507566452]}
5/5 [==============================] - 0s 3ms/step - loss: 1.6075 - acc: 0.1067